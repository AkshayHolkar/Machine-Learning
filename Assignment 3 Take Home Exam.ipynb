{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 Take Home Exam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of Ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ensemble methods, we use a combination of different models decisions to improve the performance.  Ensemble methods help to reduce the error by minimising variance and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are different methods to fuse the decisions from individual classifiers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\tBy taking average value:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this method, we take the average value of different classifiers prediction. It is one of the simple but effective ensemble method. It achieves most of benefits of reducing the variance of the estimate. For example car M’s average speed predicted by model A 70Km/hr, by model B 65km/hr and by model C 62km/hr. Then we simply take average of this number (70+65+62)/3 = 65.7km/hr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\tBy taking the majority vote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method normally use for binary classifier means have only two values to predict. In this method ensemble model simply take majority vote of different models decisions.  For example in the titanic dataset predictor, we need to predict only two value person survive or not survive. To predict the result in this method we take majority vote. Means suppose Model A predicts person survive, Model B predicts person not survive and model C predicts person survive. Then, in this case, we give a prediction that a person will survive. Since the two models have predicted that a person will survive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\tBy Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this method classifier are stacked on top of each other. So, the next classifier use to correct errors of previous classifier. It combine the predictions of several other learning algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\tBy taking weighted average of the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this method we give different weights for different models prediction base on importance. For example, we need to create predictor for web development. Data contain 40 rows. Out of 40 rows there are input for 25 rows from web professionals. And other rows input is not from professionals. Then in that case we give more importance to 25 rows which is taken from professionals then other rows. So, we give them more weightage in order to influence result by their inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two existing approaches for ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\tBagging (Bootstrap Aggregating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging or Bootstrap Aggregating is a one of advanced Ensemble technique. In this method we divide training dataset into random multiple sub-training datasets. Using each subset we create model using classifier or decision tree. At the end, we combine the result of this models by taking average value or by taking the majority vote.\n",
    " \n",
    "In this method every model take different sub-training dataset and we take their combine result which help to reduce problem of overfitting by not clinging too closely to training set. Because of that bagging method reduce variance error.\n",
    "There are many similarity between random forest and bagging. But the main difference is in bagging we take different subset for each model which further reduce various.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\tBoosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this method every model train entire training dataset. Boosting is a sequential technique, in which we train first algorithm on whole training dataset. Next algorithm pick the part which is poorly train and give more weightage to that part. Which help to reduce bias errors and build strong predictor. But it have higher changes of overfitting data which can harm our predictor. So, we need to careful while setting parameters for boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of Ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\tMore accurate result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we use multiple models in ensemble methods and combine there result for prediction. It provide more accurate result than any single model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\tStable and more robust model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of ensemble model is less noisy. Since it uses multiple models for prediction. Also, ensemble model reduces variance and bias error which make model more stable and robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\tFlexible "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble model can capture both linear and non-linear relationships in dataset. By combining two different algorithms/models result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \tDisadvantage of Ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\t Complex model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble models usually very complex. Process is normally black box. Which reduces model interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\tHigh computation and design time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since ensemble models use multiple models for prediction. It needs more computation power and take lot of time to design time. Because of this draw backs ensemble models are not good for real time application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Address of interpretability of complex diverse classifiers issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Ensemble models used multiple models to give a prediction. It have high accuracy. But the same time transparency and explainability are essential aspects of a trustful model. \n",
    "\n",
    "To make model more explainable same time keeping it’s accuracy we could add more options to constrain data in the training classifier that the why we can track the model activity. Instead of classifier add their own rules on data. We can give option to add manually which will take more time to build. But interpretability of model will be high.\n",
    "\n",
    "Out of this two options I think putting additional constrain on data of training classifier will be good idea. This method could outperform the conventional once. Because we can more sophisticated manage input data which will lead to more accuracy and clarity on model structure.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Nikunj C Oza, Kagan Tumer 2008,’Classifier ensembles: Select real-world applications‘, viewed 5 October 2019,< https://www.researchgate.net/publication/222425707_Classifier_ensembles_Select_real-world_applications >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Webb, Andrew R. ; Copsey, Keith D. 2011, ‘Ensemble Methods’, Statistical Pattern Recognition, Chapter 8, p.361-403, viewed 6 October 2019, < https://ebookcentral.proquest.com/lib/uts/detail.action?docID=819173 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaman, M. ; Hirose, Hideo 2011, ‘Classification Performance of Bagging and Boosting Type Ensemble Methods with Small Training Sets’, viewed 4 October 2019, < https://link-springer-com.ezproxy.lib.uts.edu.au/article/10.1007/s00354-011-0303-0  >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
